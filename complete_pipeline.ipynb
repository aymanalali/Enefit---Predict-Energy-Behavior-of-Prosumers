{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8007af53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:25.681007Z",
     "iopub.status.busy": "2024-07-21T09:47:25.680292Z",
     "iopub.status.idle": "2024-07-21T09:47:25.691123Z",
     "shell.execute_reply": "2024-07-21T09:47:25.690463Z"
    },
    "papermill": {
     "duration": 0.022292,
     "end_time": "2024-07-21T09:47:25.692876",
     "exception": false,
     "start_time": "2024-07-21T09:47:25.670584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_revealed_targets(revealed_targets_df):\n",
    "    # Split DataFrame based on the 'is_consumption' column\n",
    "    revealed_targets_production_df = revealed_targets_df[revealed_targets_df['is_consumption'] == False].copy()\n",
    "    revealed_targets_consumption_df = revealed_targets_df[revealed_targets_df['is_consumption'] == True].copy()\n",
    "\n",
    "    # Change column names for production DataFrame\n",
    "    revealed_targets_production_df.rename(columns={'target': 'target_production'}, inplace=True)\n",
    "\n",
    "    # Change column names for consumption DataFrame\n",
    "    revealed_targets_consumption_df.rename(columns={'target': 'target_consumption'}, inplace=True)\n",
    "\n",
    "    return revealed_targets_production_df, revealed_targets_consumption_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696a65b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:25.710272Z",
     "iopub.status.busy": "2024-07-21T09:47:25.710033Z",
     "iopub.status.idle": "2024-07-21T09:47:25.727932Z",
     "shell.execute_reply": "2024-07-21T09:47:25.727164Z"
    },
    "papermill": {
     "duration": 0.028727,
     "end_time": "2024-07-21T09:47:25.729750",
     "exception": false,
     "start_time": "2024-07-21T09:47:25.701023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ForecastWeatherProcessor:\n",
    "    def __init__(self, forecast_weather_df, points_data_file='/kaggle/input/mapping/points_data.parquet'):\n",
    "        self.forecast_weather_df = forecast_weather_df\n",
    "        self.points_data_file = points_data_file\n",
    "        self.county_mapping = {\n",
    "            \"Harjumaa\": 0,\n",
    "            \"Hiiumaa\": 1,\n",
    "            \"Ida-Virumaa\": 2,\n",
    "            \"Järvamaa\": 3,\n",
    "            \"Jõgevamaa\": 4,\n",
    "            \"Lääne-Virumaa\": 5,\n",
    "            \"Läänemaa\": 6,\n",
    "            \"Pärnumaa\": 7,\n",
    "            \"Põlvamaa\": 8,\n",
    "            \"Raplamaa\": 9,\n",
    "            \"Saaremaa\": 10,\n",
    "            \"Tartumaa\": 11,\n",
    "            None: 12,\n",
    "            \"Valgamaa\": 13,\n",
    "            \"Viljandimaa\": 14,\n",
    "            \"Võrumaa\": 15,\n",
    "        }\n",
    "        self.read_points_gdf = pd.read_parquet(self.points_data_file)\n",
    "        self.filtered_forecast_df = None\n",
    "        self.non_filtered_forecast_df = None\n",
    "\n",
    "    def map_county_id(self):\n",
    "        # Convert 'latitude' and 'longitude' to float32\n",
    "        self.read_points_gdf[['latitude', 'longitude']] = self.read_points_gdf[['latitude', 'longitude']].astype('float32')\n",
    "\n",
    "        # Map county IDs\n",
    "        self.read_points_gdf['county'] = self.read_points_gdf['county'].map(self.county_mapping)\n",
    "\n",
    "        # Merge on 'latitude', 'longitude', and convert them to float32\n",
    "        self.forecast_weather_df = pd.merge(\n",
    "            self.forecast_weather_df,\n",
    "            self.read_points_gdf[['latitude', 'longitude', 'county']].astype('float32'),\n",
    "            on=['latitude', 'longitude'],\n",
    "            how='left'\n",
    "        )\n",
    "    def filter_forecast_data(self):\n",
    "        self.forecast_weather_df.sort_values(by='origin_datetime', inplace=True)\n",
    "        unique_combinations = self.forecast_weather_df.groupby(['latitude', 'longitude', 'forecast_datetime']).cumcount() == 0\n",
    "        self.filtered_forecast_df = self.forecast_weather_df[unique_combinations]\n",
    "\n",
    "    def generate_non_filtered_forecast(self):\n",
    "        original_df = self.forecast_weather_df\n",
    "        non_filtered_df = original_df.merge(self.filtered_forecast_df,\n",
    "                                            on=['latitude', 'longitude', 'forecast_datetime', 'origin_datetime'],\n",
    "                                            how='left', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "        self.non_filtered_forecast_df = non_filtered_df\n",
    "\n",
    "    def calculate_mean_for_filtered(self):\n",
    "        grouped_df = self.filtered_forecast_df.groupby(['county', 'origin_datetime', 'forecast_datetime']).mean().reset_index()\n",
    "        self.filtered_forecast_df = grouped_df\n",
    "\n",
    "    def modify_column_names_for_filtered(self):\n",
    "        self.filtered_forecast_df.columns = [col + \"_f1\" if col != 'county' else col for col in self.filtered_forecast_df.columns]\n",
    "\n",
    "    def change_column_names_for_filtered(self):\n",
    "        self.filtered_forecast_df.rename(columns={'county_f1': 'county', 'forecast_datetime_f1': 'datetime'}, inplace=True)\n",
    "   \n",
    "    def modify_column_names_for_non_filtered(self): \n",
    "        columns_to_drop = [col for col in self.non_filtered_forecast_df.columns if col.endswith('_y')]\n",
    "        self.non_filtered_forecast_df.drop(columns=columns_to_drop, inplace=True)\n",
    "        self.non_filtered_forecast_df.columns = self.non_filtered_forecast_df.columns.str.rstrip('_x')\n",
    "   \n",
    "    def change_column_names_for_non_filtered(self):\n",
    "        self.non_filtered_forecast_df.rename(columns={'forecast_datetime_f2': 'datetime'}, inplace=True)\n",
    "   \n",
    "    \n",
    "    def add_f1_and_f2_to_column_names_for_non_filtered(self):\n",
    "        self.non_filtered_forecast_df.columns = [col + \"_f2\" if col != 'county' else col for col in self.non_filtered_forecast_df.columns]    \n",
    "    \n",
    "    def calculate_mean_for_non_filtered(self):\n",
    "        grouped_df = self.non_filtered_forecast_df.groupby(['county', 'origin_datetime_f2', 'forecast_datetime_f2']).mean().reset_index()\n",
    "        self.non_filtered_forecast_df = grouped_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def process_all_operations(self):\n",
    "        self.map_county_id()\n",
    "        self.filter_forecast_data()\n",
    "        self.generate_non_filtered_forecast()\n",
    "        self.calculate_mean_for_filtered()\n",
    "        self.modify_column_names_for_filtered()\n",
    "        self.change_column_names_for_filtered()\n",
    "        self.modify_column_names_for_non_filtered()\n",
    "        self.add_f1_and_f2_to_column_names_for_non_filtered()\n",
    "        self.calculate_mean_for_non_filtered()\n",
    "        self.change_column_names_for_non_filtered()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69eadff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:25.746626Z",
     "iopub.status.busy": "2024-07-21T09:47:25.746378Z",
     "iopub.status.idle": "2024-07-21T09:47:25.756163Z",
     "shell.execute_reply": "2024-07-21T09:47:25.755402Z"
    },
    "papermill": {
     "duration": 0.020306,
     "end_time": "2024-07-21T09:47:25.757923",
     "exception": false,
     "start_time": "2024-07-21T09:47:25.737617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HistorytWeatherProcessor:\n",
    "    def __init__(self, historical_weather_df, points_data_file='/kaggle/input/mapping/points_data.parquet'):\n",
    "        self.points_data_file = points_data_file\n",
    "        self.county_mapping = {\n",
    "            \"Harjumaa\": 0,\n",
    "            \"Hiiumaa\": 1,\n",
    "            \"Ida-Virumaa\": 2,\n",
    "            \"Järvamaa\": 3,\n",
    "            \"Jõgevamaa\": 4,\n",
    "            \"Lääne-Virumaa\": 5,\n",
    "            \"Läänemaa\": 6,\n",
    "            \"Pärnumaa\": 7,\n",
    "            \"Põlvamaa\": 8,\n",
    "            \"Raplamaa\": 9,\n",
    "            \"Saaremaa\": 10,\n",
    "            \"Tartumaa\": 11,\n",
    "            None: 12,\n",
    "            \"Valgamaa\": 13,\n",
    "            \"Viljandimaa\": 14,\n",
    "            \"Võrumaa\": 15,\n",
    "        }\n",
    "        self.read_points_gdf = pd.read_parquet(self.points_data_file)\n",
    "        self.historical_weather_df = historical_weather_df\n",
    "\n",
    "    def map_county_id(self):\n",
    "        if self.historical_weather_df is not None:\n",
    "            # Convert 'latitude' and 'longitude' to float32\n",
    "            self.read_points_gdf[['latitude', 'longitude']] = self.read_points_gdf[['latitude', 'longitude']].astype('float32')\n",
    "\n",
    "            # Map county IDs\n",
    "            self.read_points_gdf['county'] = self.read_points_gdf['county'].map(self.county_mapping)\n",
    "\n",
    "            # Merge on 'latitude', 'longitude', and convert them to float32\n",
    "            self.historical_weather_df = pd.merge(\n",
    "                self.historical_weather_df,\n",
    "                self.read_points_gdf[['latitude', 'longitude', 'county']].astype('float32'),\n",
    "                on=['latitude', 'longitude'],\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            # Convert 'county' to float32\n",
    "            self.historical_weather_df['county'] = self.historical_weather_df['county'].astype('float32')\n",
    "        else:\n",
    "            print(\"Error: historical_weather_df is not initialized.\")\n",
    "\n",
    "    def calculate_mean_for_historical_weather(self):\n",
    "        if self.historical_weather_df is not None:\n",
    "            grouped_df = self.historical_weather_df.groupby(['county', 'datetime']).mean().reset_index()\n",
    "            self.historical_weather_df = grouped_df\n",
    "        else:\n",
    "            print(\"Error: historical_weather_df is not initialized.\")\n",
    "\n",
    "    def process_all_operations(self):\n",
    "        self.map_county_id()\n",
    "        self.calculate_mean_for_historical_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f53c00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:25.775150Z",
     "iopub.status.busy": "2024-07-21T09:47:25.774889Z",
     "iopub.status.idle": "2024-07-21T09:47:26.479949Z",
     "shell.execute_reply": "2024-07-21T09:47:26.478994Z"
    },
    "papermill": {
     "duration": 0.71648,
     "end_time": "2024-07-21T09:47:26.482323",
     "exception": false,
     "start_time": "2024-07-21T09:47:25.765843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# after you fininsh check files and columns names to be idetical to the ones in the api \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class ConcatenateDataFrames:\n",
    "    def __init__(self, test_df, electricity_prices_df, historical_weather_df, *additional_dfs):\n",
    "        self.test_df = test_df  # Replaced train_df with test_df\n",
    "        self.electricity_prices_df = electricity_prices_df\n",
    "        self.historical_weather_df1 = historical_weather_df1\n",
    "        self.additional_dfs = additional_dfs\n",
    "        self.filtered_df = filtered_df\n",
    "        self.non_filtered_df = non_filtered_df\n",
    "        self.gas_prices_df = gas_prices_df\n",
    "        self.client_df = client_df\n",
    "        self.revealed_targets_consumption = revealed_targets_consumption\n",
    "        self.revealed_targets_production = revealed_targets_production\n",
    "        \n",
    "    def concatenate_dfs(self):\n",
    "        # Convert relevant columns to appropriate data types\n",
    "        self.test_df['prediction_datetime'] = pd.to_datetime(self.test_df['prediction_datetime'])\n",
    "        self.test_df['date'] = pd.to_datetime(self.test_df['prediction_datetime']).dt.date  # Add this line\n",
    "        self.electricity_prices_df['forecast_date'] = pd.to_datetime(self.electricity_prices_df['forecast_date'])\n",
    "        self.historical_weather_df1['datetime'] = pd.to_datetime(self.historical_weather_df1['datetime'])\n",
    "        self.non_filtered_df['datetime'] = pd.to_datetime(self.non_filtered_df['datetime'])\n",
    "        self.filtered_df['datetime'] = pd.to_datetime(self.filtered_df['datetime'])\n",
    "        self.gas_prices_df['forecast_date'] = pd.to_datetime(self.gas_prices_df['forecast_date'])\n",
    "        self.client_df['date'] = pd.to_datetime(self.client_df['date'])\n",
    "        self.test_df['date'] = pd.to_datetime(self.test_df['date'])\n",
    "        self.revealed_targets_consumption['datetime'] = pd.to_datetime(self.revealed_targets_consumption['datetime'])\n",
    "        self.revealed_targets_production['datetime'] = pd.to_datetime(self.revealed_targets_production['datetime'])\n",
    "\n",
    "\n",
    "        \n",
    "        # Joining test_df and electricity_prices_df based on forecast_date and prediction_datetime with a lag of one day\n",
    "        merged_df = pd.merge(\n",
    "            self.test_df,  # Replaced train_df with test_df\n",
    "            self.electricity_prices_df,\n",
    "            left_on='prediction_datetime',\n",
    "            right_on=self.electricity_prices_df['forecast_date'] + pd.DateOffset(1),\n",
    "            how='left',\n",
    "            suffixes=('', '_electricity_prices')\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Joining test_df and gas_prices_df based on forecast_date and prediction_datetime with a lag of one day\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            self.gas_prices_df,\n",
    "            left_on='date',\n",
    "            right_on=self.gas_prices_df['forecast_date'] + pd.DateOffset(1),\n",
    "            how='left',\n",
    "            suffixes=('', '_gas_prices')\n",
    "        )        \n",
    "        \n",
    "        # Drop duplicate columns after the merge\n",
    "        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "       \n",
    "        # Joining merged_df and historical_weather_df based on datetime and county with a lag of two days\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            self.historical_weather_df1,\n",
    "            left_on=['county', 'prediction_datetime'],\n",
    "            right_on=[self.historical_weather_df1['county'], self.historical_weather_df1['datetime'] + pd.DateOffset(2)],\n",
    "            how='left',\n",
    "            suffixes=('', '_historical_weather')\n",
    "\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Joining merged_df and filtered_df based on datetime and county with a lag of two days\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            self.filtered_df,\n",
    "            left_on=['county', 'prediction_datetime'],\n",
    "            right_on=[self.filtered_df['county'], self.filtered_df['datetime'] ],\n",
    "            how='left',\n",
    "            suffixes=('', '_filtered')\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Joining merged_df and non_filtered_df based on datetime and county with a lag of two days\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            self.non_filtered_df,\n",
    "            left_on=['county', 'prediction_datetime'],\n",
    "            right_on=[self.non_filtered_df['county'], self.non_filtered_df['datetime'] + pd.DateOffset(1)],\n",
    "            how='left',\n",
    "            suffixes=('', '_non_filtered')\n",
    "        )\n",
    "        \n",
    "        # Joining merged_df and self.client_df based on product_type, county, is_business, and date with a lag of one day\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            self.client_df,\n",
    "            left_on=['product_type', 'county', 'is_business', 'date'],\n",
    "            right_on=['product_type', 'county', 'is_business', self.client_df['date'] + pd.DateOffset(2)],\n",
    "            how='left',\n",
    "            suffixes=('', '_client_df')\n",
    "        )\n",
    "        \n",
    "        # Joining merged_df and self.revealed_targets_production based on product_type, county, is_business, and date with a lag of one day\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            self.revealed_targets_production,\n",
    "            left_on=['product_type', 'county', 'is_business', 'prediction_datetime'],\n",
    "            right_on=['product_type', 'county', 'is_business', self.revealed_targets_production['datetime'] + pd.DateOffset(2)],\n",
    "            how='left',\n",
    "            suffixes=('', 'revealed_targets_production')\n",
    "        )\n",
    "        # Joining merged_df and self.revealed_targets_consumption based on product_type, county, is_business, and date with a lag of one day\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            self.revealed_targets_consumption,\n",
    "            left_on=['product_type', 'county', 'is_business', 'prediction_datetime'],\n",
    "            right_on=['product_type', 'county', 'is_business', self.revealed_targets_consumption['datetime'] + pd.DateOffset(2)],\n",
    "            how='left',\n",
    "            suffixes=('', 'revealed_targets_consumption')      \n",
    "            \n",
    "        )        \n",
    "        \n",
    "        \n",
    "        # Drop duplicate columns after the merge\n",
    "        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "\n",
    "\n",
    "\n",
    "        return merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "576b2904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:26.500015Z",
     "iopub.status.busy": "2024-07-21T09:47:26.499417Z",
     "iopub.status.idle": "2024-07-21T09:47:26.510484Z",
     "shell.execute_reply": "2024-07-21T09:47:26.509645Z"
    },
    "papermill": {
     "duration": 0.021758,
     "end_time": "2024-07-21T09:47:26.512352",
     "exception": false,
     "start_time": "2024-07-21T09:47:26.490594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode_datetime_columns(result_df):\n",
    "    # Extract time, day of year, and day of week from the datetime column\n",
    "    result_df['time'] = result_df['datetime'].dt.time\n",
    "    result_df['day_of_year'] = result_df['datetime'].dt.dayofyear\n",
    "    result_df['day_of_week'] = result_df['datetime'].dt.dayofweek\n",
    "\n",
    "    # Extract month of the year and day of the month\n",
    "    result_df['month_of_year'] = result_df['datetime'].dt.month\n",
    "    result_df['day_of_month'] = result_df['datetime'].dt.day\n",
    "\n",
    "    # Apply cyclic encoding to generate sin and cos columns\n",
    "    result_df['time_sin'] = np.sin(2 * np.pi * result_df['time'].apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second) / 86400)\n",
    "    result_df['time_cos'] = np.cos(2 * np.pi * result_df['time'].apply(lambda x: x.hour * 3600 + x.minute * 60 + x.second) / 86400)\n",
    "\n",
    "    result_df['day_of_year_sin'] = np.sin(2 * np.pi * result_df['day_of_year'] / 365)\n",
    "    result_df['day_of_year_cos'] = np.cos(2 * np.pi * result_df['day_of_year'] / 365)\n",
    "\n",
    "    result_df['day_of_week_sin'] = np.sin(2 * np.pi * result_df['day_of_week'] / 7)\n",
    "    result_df['day_of_week_cos'] = np.cos(2 * np.pi * result_df['day_of_week'] / 7)\n",
    "\n",
    "    result_df['month_of_year_sin'] = np.sin(2 * np.pi * result_df['month_of_year'] / 12)\n",
    "    result_df['month_of_year_cos'] = np.cos(2 * np.pi * result_df['month_of_year'] / 12)\n",
    "\n",
    "    result_df['day_of_month_sin'] = np.sin(2 * np.pi * result_df['day_of_month'] / 31)\n",
    "    result_df['day_of_month_cos'] = np.cos(2 * np.pi * result_df['day_of_month'] / 31)\n",
    "\n",
    "    # Drop the original datetime, time, day of year, day of week, month of the year, and day of the month columns\n",
    "    result_df.drop(columns=['time', 'day_of_year', 'day_of_week', 'month_of_year', 'day_of_month'], inplace=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6ba9fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:26.529357Z",
     "iopub.status.busy": "2024-07-21T09:47:26.528737Z",
     "iopub.status.idle": "2024-07-21T09:47:26.535293Z",
     "shell.execute_reply": "2024-07-21T09:47:26.534516Z"
    },
    "papermill": {
     "duration": 0.017004,
     "end_time": "2024-07-21T09:47:26.537218",
     "exception": false,
     "start_time": "2024-07-21T09:47:26.520214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_columns(result_df, selected_columns):\n",
    "    # Create a new DataFrame with only the selected columns\n",
    "    result_df_version = result_df[selected_columns].copy()\n",
    "\n",
    "    return result_df_version\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'result_df' is your DataFrame and 'selected_columns' is the list of columns to extract\n",
    "\n",
    "selected_columns = ['prediction_datetime', 'mean_price', 'euros_per_mwh', 'temperature', 'dewpoint',\n",
    "                    'shortwave_radiation', 'direct_solar_radiation',\n",
    "                    'diffuse_radiation', 'latitude', 'longitude', 'temperature_f1',\n",
    "                    'dewpoint_f1','day_of_month_cos', 'day_of_month_sin', 'month_of_year_cos',\n",
    "                    'month_of_year_sin', 'currently_scored',\n",
    "                    'surface_solar_radiation_downwards_f1', 'temperature_f2', 'dewpoint_f2',\n",
    "                    'direct_solar_radiation_f2', 'direct_solar_radiation_f1', \n",
    "                    'surface_solar_radiation_downwards_f2', 'eic_count', 'installed_capacity',\n",
    "                    'target_production', 'target_consumption', 'time_sin', 'time_cos', 'day_of_year_sin', 'day_of_year_cos', \n",
    "                    'day_of_week_sin', 'day_of_week_cos', 'is_consumption', 'prediction_unit_id', 'row_id', \n",
    "                    'is_business', 'product_type', 'rain', 'snowfall', 'surface_pressure','snowfall_f1', \n",
    "                    'total_precipitation_f1', 'snowfall_f2', 'total_precipitation_f2', 'windspeed_10m', 'winddirection_10m',\n",
    "                   '10_metre_u_wind_component_f1', '10_metre_v_wind_component_f1', '10_metre_u_wind_component_f2', '10_metre_v_wind_component_f2'\n",
    "                   , 'cloudcover_low', 'cloudcover_low_f1','cloudcover_low_f2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27810da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:26.554006Z",
     "iopub.status.busy": "2024-07-21T09:47:26.553736Z",
     "iopub.status.idle": "2024-07-21T09:47:27.378565Z",
     "shell.execute_reply": "2024-07-21T09:47:27.377812Z"
    },
    "papermill": {
     "duration": 0.835733,
     "end_time": "2024-07-21T09:47:27.380909",
     "exception": false,
     "start_time": "2024-07-21T09:47:26.545176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_columns(result_df, columns_to_normalize): \n",
    "    # Create a MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Normalize specified columns\n",
    "    result_df[columns_to_normalize] = scaler.fit_transform(result_df[columns_to_normalize])\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'result_df' is your DataFrame and 'columns_to_normalize' is the list of columns to normalize\n",
    "\n",
    "columns_to_normalize = ['mean_price', 'euros_per_mwh', 'temperature', 'dewpoint',\n",
    "                          'shortwave_radiation', 'direct_solar_radiation',\n",
    "                         'diffuse_radiation', 'latitude', 'longitude', 'temperature_f1',\n",
    "                         'dewpoint_f1',  'direct_solar_radiation_f1',\n",
    "                         'surface_solar_radiation_downwards_f1', 'temperature_f2', 'dewpoint_f2',\n",
    "                          'direct_solar_radiation_f2',\n",
    "                         'surface_solar_radiation_downwards_f2', 'eic_count', 'installed_capacity',\n",
    "                         'target_production', 'target_consumption', 'rain', \n",
    "                        'snowfall', 'surface_pressure','snowfall_f1', 'total_precipitation_f1', 'snowfall_f2', 'total_precipitation_f2'\n",
    "                       , 'windspeed_10m', 'winddirection_10m', '10_metre_u_wind_component_f1', '10_metre_v_wind_component_f1', \n",
    "                        '10_metre_u_wind_component_f2', '10_metre_v_wind_component_f2', 'cloudcover_low', 'cloudcover_low_f1', 'cloudcover_low_f2']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a145132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:27.399009Z",
     "iopub.status.busy": "2024-07-21T09:47:27.398695Z",
     "iopub.status.idle": "2024-07-21T09:47:27.406127Z",
     "shell.execute_reply": "2024-07-21T09:47:27.405249Z"
    },
    "papermill": {
     "duration": 0.018559,
     "end_time": "2024-07-21T09:47:27.408129",
     "exception": false,
     "start_time": "2024-07-21T09:47:27.389570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def fill_nans(df):\n",
    "    # Convert 'prediction_datetime' to datetime type if not already\n",
    "    if not pd.api.types.is_datetime64_ns_dtype(df['prediction_datetime']):\n",
    "        df['prediction_datetime'] = pd.to_datetime(df['prediction_datetime'])\n",
    "    \n",
    "    # Sort the DataFrame based on prediction_datetime\n",
    "    df.sort_values(by=['prediction_datetime'], inplace=True)\n",
    "\n",
    "    # Iterate over columns and fill NaN with the average of the same hour for the last week\n",
    "    for column in df.columns.difference(['prediction_datetime', 'currently_scored']):\n",
    "        df[column] = df.groupby(df['prediction_datetime'].dt.hour)[column].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    # Fill NaN values for boolean column 'currently_scored' with False\n",
    "    df['currently_scored'].fillna(False, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_df(df):\n",
    "    \"\"\"Processes a DataFrame and saves it back to a CSV file with the same name.\"\"\"\n",
    "\n",
    "    df = fill_nans(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "557f967a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:27.426021Z",
     "iopub.status.busy": "2024-07-21T09:47:27.425737Z",
     "iopub.status.idle": "2024-07-21T09:47:27.430716Z",
     "shell.execute_reply": "2024-07-21T09:47:27.429935Z"
    },
    "papermill": {
     "duration": 0.016597,
     "end_time": "2024-07-21T09:47:27.432646",
     "exception": false,
     "start_time": "2024-07-21T09:47:27.416049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_csv_by_consumption(filename):\n",
    "    \"\"\"Splits a CSV file based on the 'is_consumption' column and returns two DataFrames.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the original CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two pandas DataFrames:\n",
    "            - result_df_consumption: DataFrame with rows where is_consumption == 1\n",
    "            - result_df_production: DataFrame with rows where is_consumption == 0\n",
    "    \"\"\"\n",
    "\n",
    "    df = filename\n",
    "\n",
    "    # Filter data based on is_consumption values\n",
    "    result_df_consumption = df[df['is_consumption'] == 1]\n",
    "    result_df_production = df[df['is_consumption'] == 0]\n",
    "\n",
    "    return result_df_consumption, result_df_production\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0291c0e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:27.449775Z",
     "iopub.status.busy": "2024-07-21T09:47:27.449481Z",
     "iopub.status.idle": "2024-07-21T09:47:27.460291Z",
     "shell.execute_reply": "2024-07-21T09:47:27.459481Z"
    },
    "papermill": {
     "duration": 0.021618,
     "end_time": "2024-07-21T09:47:27.462215",
     "exception": false,
     "start_time": "2024-07-21T09:47:27.440597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_features_for_unit(df, unit_id):\n",
    "    df_unit = df[df['prediction_unit_id'] == unit_id].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Create delay features\n",
    "    for delay in (1, 24, 48, 72, 168, 336, 672):\n",
    "        for feature in ['target_production', 'target_consumption', 'euros_per_mwh', 'mean_price']:\n",
    "            new_feature_name = f\"{feature}_delay_{delay}hrs\"\n",
    "            df_unit.loc[:, new_feature_name] = df_unit[feature].shift(delay)\n",
    "\n",
    "    # Create rolling window features\n",
    "    for window in (24, 48, 168, 672):\n",
    "        for feature in ['target_production', 'target_consumption', 'euros_per_mwh', 'mean_price']:\n",
    "            new_feature_name = f\"{feature}_RW_{window}hrs\"\n",
    "            df_unit.loc[:, new_feature_name] = df_unit[feature].rolling(window=window).mean()\n",
    "\n",
    "    # Additional features with delays (1, 24, 48, 72)hrs and RW(24, 48, 72)hrs\n",
    "    additional_features = [\n",
    "        'temperature', 'dewpoint', 'shortwave_radiation', 'windspeed_10m',\n",
    "        'winddirection_10m', 'rain', 'snowfall', 'surface_pressure',\n",
    "        'direct_solar_radiation', 'diffuse_radiation', 'cloudcover_low'\n",
    "    ]\n",
    "    delays = (1, 24, 48, 72)\n",
    "    windows = (24, 48, 72)\n",
    "    for feature in additional_features:\n",
    "        for delay in delays:\n",
    "            new_feature_name = f\"{feature}_delay_{delay}hrs\"\n",
    "            df_unit.loc[:, new_feature_name] = df_unit[feature].shift(delay)\n",
    "        for window in windows:\n",
    "            new_feature_name = f\"{feature}_RW_{window}hrs\"\n",
    "            df_unit.loc[:, new_feature_name] = df_unit[feature].rolling(window=window).mean()\n",
    "\n",
    "    # Additional features with 1hr delay and RW(24hrs)\n",
    "    more_features = [\n",
    "        'temperature_f1', 'dewpoint_f1', 'direct_solar_radiation_f1',\n",
    "        'surface_solar_radiation_downwards_f1', 'temperature_f2', 'dewpoint_f2',\n",
    "        'direct_solar_radiation_f2', 'surface_solar_radiation_downwards_f2',\n",
    "        'snowfall_f1', 'total_precipitation_f1', 'snowfall_f2',\n",
    "        'total_precipitation_f2', '10_metre_u_wind_component_f1',\n",
    "        '10_metre_v_wind_component_f1', '10_metre_u_wind_component_f2',\n",
    "        '10_metre_v_wind_component_f2', 'cloudcover_low_f1', 'cloudcover_low_f2'\n",
    "    ]\n",
    "    for feature in more_features:\n",
    "        df_unit.loc[:, f\"{feature}_delay_1hrs\"] = df_unit[feature].shift(1)\n",
    "        df_unit.loc[:, f\"{feature}_RW_24hrs\"] = df_unit[feature].rolling(window=24).mean()\n",
    "\n",
    "    return df_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa15992b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:27.478970Z",
     "iopub.status.busy": "2024-07-21T09:47:27.478709Z",
     "iopub.status.idle": "2024-07-21T09:47:27.483001Z",
     "shell.execute_reply": "2024-07-21T09:47:27.482177Z"
    },
    "papermill": {
     "duration": 0.014662,
     "end_time": "2024-07-21T09:47:27.484773",
     "exception": false,
     "start_time": "2024-07-21T09:47:27.470111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_bool_columns_to_int64(dataframe):\n",
    "    bool_columns = dataframe.select_dtypes(include='bool').columns\n",
    "    dataframe[bool_columns] = dataframe[bool_columns].astype('int64')\n",
    "    return dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4af026b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:27.501381Z",
     "iopub.status.busy": "2024-07-21T09:47:27.501141Z",
     "iopub.status.idle": "2024-07-21T09:47:27.505600Z",
     "shell.execute_reply": "2024-07-21T09:47:27.504824Z"
    },
    "papermill": {
     "duration": 0.014779,
     "end_time": "2024-07-21T09:47:27.507415",
     "exception": false,
     "start_time": "2024-07-21T09:47:27.492636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_l_espoir(year):\n",
    "    if year == 2021:\n",
    "        return 0\n",
    "    elif year == 2022:\n",
    "        return 1 \n",
    "    elif year == 2023:\n",
    "        return  2\n",
    "    elif year == 2024:\n",
    "        return  3\n",
    "    else:\n",
    "        return None  # Adjust this according to your needs if other years may occur\n",
    "\n",
    "# Apply the function to create the 'l_espoir' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16cdbb0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:27.524128Z",
     "iopub.status.busy": "2024-07-21T09:47:27.523884Z",
     "iopub.status.idle": "2024-07-21T09:47:31.545242Z",
     "shell.execute_reply": "2024-07-21T09:47:31.544445Z"
    },
    "papermill": {
     "duration": 4.032301,
     "end_time": "2024-07-21T09:47:31.547545",
     "exception": false,
     "start_time": "2024-07-21T09:47:27.515244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "production_model = lgb.Booster(model_file=\"/kaggle/input/pleasebebetter/pro.txt\")\n",
    "consumption_model = lgb.Booster(model_file=\"/kaggle/input/pleasebebetter/con.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbcce547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:31.565522Z",
     "iopub.status.busy": "2024-07-21T09:47:31.565248Z",
     "iopub.status.idle": "2024-07-21T09:47:31.580878Z",
     "shell.execute_reply": "2024-07-21T09:47:31.580020Z"
    },
    "papermill": {
     "duration": 0.026714,
     "end_time": "2024-07-21T09:47:31.582815",
     "exception": false,
     "start_time": "2024-07-21T09:47:31.556101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def update_with_new_data(\n",
    "        df_client,\n",
    "        df_gas_prices,\n",
    "        df_electricity_prices,\n",
    "        df_forecast_weather,\n",
    "        df_historical_weather,\n",
    "        df_target,\n",
    "        df_test,\n",
    "        df_new_client,\n",
    "        df_new_gas_prices,\n",
    "        df_new_electricity_prices,\n",
    "        df_new_forecast_weather,\n",
    "        df_new_historical_weather,\n",
    "        df_new_target,\n",
    "        df_new_test\n",
    "):\n",
    "    # Convert new DataFrames to Pandas DataFrames\n",
    "    # List of columns to exclude\n",
    "    exclude_columns = ['data_block_id']\n",
    "\n",
    "    # Exclude 'data_block_id' from each DataFrame\n",
    "    df_new_client = df_new_client[[col for col in df_client.columns if col not in exclude_columns]]\n",
    "    df_new_gas_prices = df_new_gas_prices[[col for col in df_gas_prices.columns if col not in exclude_columns]]\n",
    "    df_new_electricity_prices = df_new_electricity_prices[[col for col in df_electricity_prices.columns if col not in exclude_columns]]\n",
    "    df_new_forecast_weather = df_new_forecast_weather[[col for col in df_forecast_weather.columns if col not in exclude_columns]]\n",
    "    df_new_historical_weather = df_new_historical_weather[[col for col in df_historical_weather.columns if col not in exclude_columns]]\n",
    "    df_new_target = df_new_target[[col for col in df_target.columns if col not in exclude_columns]]\n",
    "    df_new_test = df_new_test[[col for col in df_test.columns if col not in exclude_columns]]\n",
    "\n",
    "    # Convert datetime columns to datetime format\n",
    "    datetime_columns = ['prediction_datetime', 'date', 'forecast_date', 'datetime', 'forecast_datetime', 'original_datetime']\n",
    "    for df in [df_client,\n",
    "        df_gas_prices,\n",
    "        df_electricity_prices,\n",
    "        df_forecast_weather,\n",
    "        df_historical_weather,\n",
    "        df_target,\n",
    "        df_test,df_new_client, df_new_gas_prices, df_new_electricity_prices, df_new_forecast_weather, df_new_historical_weather, df_new_target, df_new_test]:\n",
    "        for col in datetime_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "                \n",
    "                \n",
    "\n",
    "    dfs_to_convert = [\n",
    "    \"df_client\",\n",
    "    \"df_gas_prices\",\n",
    "    \"df_electricity_prices\",\n",
    "    \"df_forecast_weather\",\n",
    "    \"df_historical_weather\",\n",
    "    \"df_target\",\n",
    "    \"df_test\",\n",
    "    \"df_new_client\",\n",
    "    \"df_new_gas_prices\",\n",
    "    \"df_new_electricity_prices\",\n",
    "    \"df_new_forecast_weather\",\n",
    "    \"df_new_historical_weather\",\n",
    "    \"df_new_target\",\n",
    "    \"df_new_test\",\n",
    "    ]\n",
    "\n",
    "    dataframes = [df_client, df_gas_prices, df_electricity_prices, df_forecast_weather, df_historical_weather, df_target, df_test,\n",
    "                   df_new_client, df_new_gas_prices, df_new_electricity_prices, df_new_forecast_weather, df_new_historical_weather, df_new_target, df_new_test]\n",
    "\n",
    "    for df in dataframes:\n",
    "        df[df.select_dtypes(include=['float64']).columns] = df.select_dtypes(include=['float64']).astype('float32')\n",
    "            \n",
    "\n",
    "    # Concatenate new data with existing data and update existing DataFrames\n",
    "    df_client = pd.concat([df_client, df_new_client]).drop_duplicates(subset=[\"date\", \"county\", \"is_business\", \"product_type\"])\n",
    "    df_gas_prices = pd.concat([df_gas_prices, df_new_gas_prices]).drop_duplicates(subset=[\"forecast_date\"])\n",
    "    df_electricity_prices = pd.concat([df_electricity_prices, df_new_electricity_prices]).drop_duplicates(subset=[\"forecast_date\"])\n",
    "    df_forecast_weather = pd.concat([df_forecast_weather, df_new_forecast_weather]).drop_duplicates(subset=[\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n",
    "    df_historical_weather = pd.concat([df_historical_weather, df_new_historical_weather]).drop_duplicates(subset=[\"datetime\", \"latitude\", \"longitude\"])\n",
    "    df_target = pd.concat([df_target, df_new_target]).drop_duplicates(subset=[\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"])\n",
    "    df_test = pd.concat([df_test, df_new_test]).drop_duplicates(subset=[\"prediction_datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"])\n",
    "    \n",
    "    return df_client, df_gas_prices, df_electricity_prices, df_forecast_weather, df_historical_weather, df_target, df_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "985f29f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:31.599900Z",
     "iopub.status.busy": "2024-07-21T09:47:31.599598Z",
     "iopub.status.idle": "2024-07-21T09:47:57.046235Z",
     "shell.execute_reply": "2024-07-21T09:47:57.045433Z"
    },
    "papermill": {
     "duration": 25.457839,
     "end_time": "2024-07-21T09:47:57.048627",
     "exception": false,
     "start_time": "2024-07-21T09:47:31.590788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "client_df = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/client.csv\")\n",
    "gas_prices_df = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv\")\n",
    "electricity_prices_df = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv\")\n",
    "forecast_weather_df = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv\")\n",
    "historical_weather_df = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv\")\n",
    "revealed_targets_df = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c3daeb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:57.066612Z",
     "iopub.status.busy": "2024-07-21T09:47:57.066331Z",
     "iopub.status.idle": "2024-07-21T09:47:57.465549Z",
     "shell.execute_reply": "2024-07-21T09:47:57.464733Z"
    },
    "papermill": {
     "duration": 0.410871,
     "end_time": "2024-07-21T09:47:57.468010",
     "exception": false,
     "start_time": "2024-07-21T09:47:57.057139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of DataFrames\n",
    "dfs = [client_df, gas_prices_df, electricity_prices_df, forecast_weather_df, historical_weather_df, revealed_targets_df, test_df]\n",
    "\n",
    "# Column to drop\n",
    "column_to_drop = 'data_block_id'\n",
    "\n",
    "# Drop the column from each DataFrame\n",
    "for df in dfs:\n",
    "    if column_to_drop in df.columns:\n",
    "        df.drop(column_to_drop, axis=1, inplace=True) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6679ede9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:57.486261Z",
     "iopub.status.busy": "2024-07-21T09:47:57.485943Z",
     "iopub.status.idle": "2024-07-21T09:47:57.563604Z",
     "shell.execute_reply": "2024-07-21T09:47:57.562629Z"
    },
    "papermill": {
     "duration": 0.08929,
     "end_time": "2024-07-21T09:47:57.566133",
     "exception": false,
     "start_time": "2024-07-21T09:47:57.476843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rename 'datetime' to 'prediction_datetime' in df_test\n",
    "test_df.rename(columns={'datetime': 'prediction_datetime'}, inplace=True)\n",
    "\n",
    "# Drop 'target' column in df_test \n",
    "test_df.drop(columns=['target'], inplace=True)\n",
    "\n",
    "test_df['currently_scored'] = 0\n",
    "\n",
    "# If you want the new column to be of type int64 explicitly, you can cast it\n",
    "test_df['currently_scored'] = test_df['currently_scored'].astype('int64')\n",
    "\n",
    "gas_prices_df['mean_price'] = gas_prices_df[['lowest_price_per_mwh', 'highest_price_per_mwh']].mean(axis=1)\n",
    "gas_prices_df = gas_prices_df.drop(['lowest_price_per_mwh', 'highest_price_per_mwh'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "765c4c16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:57.583800Z",
     "iopub.status.busy": "2024-07-21T09:47:57.583462Z",
     "iopub.status.idle": "2024-07-21T09:47:57.605860Z",
     "shell.execute_reply": "2024-07-21T09:47:57.605005Z"
    },
    "papermill": {
     "duration": 0.033333,
     "end_time": "2024-07-21T09:47:57.607823",
     "exception": false,
     "start_time": "2024-07-21T09:47:57.574490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import enefit \n",
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24867deb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:57.625383Z",
     "iopub.status.busy": "2024-07-21T09:47:57.624908Z",
     "iopub.status.idle": "2024-07-21T09:47:57.630633Z",
     "shell.execute_reply": "2024-07-21T09:47:57.629830Z"
    },
    "papermill": {
     "duration": 0.016532,
     "end_time": "2024-07-21T09:47:57.632521",
     "exception": false,
     "start_time": "2024-07-21T09:47:57.615989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def drop_first_n_days(df, date_column_name, n=3):\n",
    "    # Convert the date column to datetime if it's not already in datetime format\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[date_column_name]):\n",
    "        df[date_column_name] = pd.to_datetime(df[date_column_name], errors='coerce')\n",
    "\n",
    "    # Identify the earliest date\n",
    "    earliest_date = df[date_column_name].min()\n",
    "\n",
    "    # Calculate the date threshold for dropping data \n",
    "    threshold_date = earliest_date + pd.Timedelta(days=n)\n",
    "\n",
    "    # Filter data to keep only rows with dates after the threshold date\n",
    "    filtered_df = df[df[date_column_name] >= threshold_date]\n",
    "\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4e99d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:57.649668Z",
     "iopub.status.busy": "2024-07-21T09:47:57.649413Z",
     "iopub.status.idle": "2024-07-21T09:47:57.660540Z",
     "shell.execute_reply": "2024-07-21T09:47:57.659725Z"
    },
    "papermill": {
     "duration": 0.021905,
     "end_time": "2024-07-21T09:47:57.662342",
     "exception": false,
     "start_time": "2024-07-21T09:47:57.640437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"target_production_delay_1hrs\", \"target_consumption_delay_1hrs\", \"euros_per_mwh_delay_1hrs\", \"mean_price_delay_1hrs\",\n",
    "    \"target_production_delay_24hrs\", \"target_consumption_delay_24hrs\", \"euros_per_mwh_delay_24hrs\", \"mean_price_delay_24hrs\",\n",
    "    \"target_production_delay_48hrs\", \"target_consumption_delay_48hrs\", \"euros_per_mwh_delay_48hrs\", \"mean_price_delay_48hrs\",\n",
    "    \"target_production_delay_72hrs\", \"target_consumption_delay_72hrs\", \"euros_per_mwh_delay_72hrs\", \"mean_price_delay_72hrs\",\n",
    "    \"target_production_delay_168hrs\", \"target_consumption_delay_168hrs\", \"euros_per_mwh_delay_168hrs\", \"mean_price_delay_168hrs\",\n",
    "    \"target_production_delay_336hrs\", \"target_consumption_delay_336hrs\", \"euros_per_mwh_delay_336hrs\", \"mean_price_delay_336hrs\",\n",
    "    \"target_production_delay_672hrs\", \"target_consumption_delay_672hrs\", \"euros_per_mwh_delay_672hrs\", \"mean_price_delay_672hrs\",\n",
    "    \"target_production_RW_24hrs\", \"target_consumption_RW_24hrs\", \"euros_per_mwh_RW_24hrs\", \"mean_price_RW_24hrs\",\n",
    "    \"target_production_RW_48hrs\", \"target_consumption_RW_48hrs\", \"euros_per_mwh_RW_48hrs\", \"mean_price_RW_48hrs\",\n",
    "    \"target_production_RW_168hrs\", \"target_consumption_RW_168hrs\", \"euros_per_mwh_RW_168hrs\", \"mean_price_RW_168hrs\",\n",
    "    \"target_production_RW_672hrs\", \"target_consumption_RW_672hrs\", \"euros_per_mwh_RW_672hrs\", \"mean_price_RW_672hrs\",\n",
    "    \"temperature_delay_1hrs\", \"temperature_delay_24hrs\", \"temperature_delay_48hrs\", \"temperature_delay_72hrs\",\n",
    "    \"temperature_RW_24hrs\", \"temperature_RW_48hrs\", \"temperature_RW_72hrs\",\n",
    "    \"dewpoint_delay_1hrs\", \"dewpoint_delay_24hrs\", \"dewpoint_delay_48hrs\", \"dewpoint_delay_72hrs\",\n",
    "    \"dewpoint_RW_24hrs\", \"dewpoint_RW_48hrs\", \"dewpoint_RW_72hrs\",\n",
    "    \"shortwave_radiation_delay_1hrs\", \"shortwave_radiation_delay_24hrs\", \"shortwave_radiation_delay_48hrs\", \"shortwave_radiation_delay_72hrs\",\n",
    "    \"shortwave_radiation_RW_24hrs\", \"shortwave_radiation_RW_48hrs\", \"shortwave_radiation_RW_72hrs\",\n",
    "    \"windspeed_10m_delay_1hrs\", \"windspeed_10m_delay_24hrs\", \"windspeed_10m_delay_48hrs\", \"windspeed_10m_delay_72hrs\",\n",
    "    \"windspeed_10m_RW_24hrs\", \"windspeed_10m_RW_48hrs\", \"windspeed_10m_RW_72hrs\",\n",
    "    \"winddirection_10m_delay_1hrs\", \"winddirection_10m_delay_24hrs\", \"winddirection_10m_delay_48hrs\", \"winddirection_10m_delay_72hrs\",\n",
    "    \"winddirection_10m_RW_24hrs\", \"winddirection_10m_RW_48hrs\", \"winddirection_10m_RW_72hrs\",\n",
    "    \"rain_delay_1hrs\", \"rain_delay_24hrs\", \"rain_delay_48hrs\", \"rain_delay_72hrs\",\n",
    "    \"rain_RW_24hrs\", \"rain_RW_48hrs\", \"rain_RW_72hrs\",\n",
    "    \"snowfall_delay_1hrs\", \"snowfall_delay_24hrs\", \"snowfall_delay_48hrs\", \"snowfall_delay_72hrs\",\n",
    "    \"snowfall_RW_24hrs\", \"snowfall_RW_48hrs\", \"snowfall_RW_72hrs\",\n",
    "    \"surface_pressure_delay_1hrs\", \"surface_pressure_delay_24hrs\", \"surface_pressure_delay_48hrs\", \"surface_pressure_delay_72hrs\",\n",
    "    \"surface_pressure_RW_24hrs\", \"surface_pressure_RW_48hrs\", \"surface_pressure_RW_72hrs\",\n",
    "    \"direct_solar_radiation_delay_1hrs\", \"direct_solar_radiation_delay_24hrs\", \"direct_solar_radiation_delay_48hrs\", \"direct_solar_radiation_delay_72hrs\",\n",
    "    \"direct_solar_radiation_RW_24hrs\", \"direct_solar_radiation_RW_48hrs\", \"direct_solar_radiation_RW_72hrs\",\n",
    "    \"diffuse_radiation_delay_1hrs\", \"diffuse_radiation_delay_24hrs\", \"diffuse_radiation_delay_48hrs\", \"diffuse_radiation_delay_72hrs\",\n",
    "    \"diffuse_radiation_RW_24hrs\", \"diffuse_radiation_RW_48hrs\", \"diffuse_radiation_RW_72hrs\",\n",
    "    \"cloudcover_low_delay_1hrs\", \"cloudcover_low_delay_24hrs\", \"cloudcover_low_delay_48hrs\", \"cloudcover_low_delay_72hrs\",\n",
    "    \"cloudcover_low_RW_24hrs\", \"cloudcover_low_RW_48hrs\", \"cloudcover_low_RW_72hrs\",\n",
    "    \"temperature_f1_delay_1hrs\", \"temperature_f1_RW_24hrs\",\n",
    "    \"dewpoint_f1_delay_1hrs\", \"dewpoint_f1_RW_24hrs\",\n",
    "    \"direct_solar_radiation_f1_delay_1hrs\", \"direct_solar_radiation_f1_RW_24hrs\",\n",
    "    \"surface_solar_radiation_downwards_f1_delay_1hrs\", \"surface_solar_radiation_downwards_f1_RW_24hrs\",\n",
    "    \"temperature_f2_delay_1hrs\", \"temperature_f2_RW_24hrs\",\n",
    "    \"dewpoint_f2_delay_1hrs\", \"dewpoint_f2_RW_24hrs\",\n",
    "    \"direct_solar_radiation_f2_delay_1hrs\", \"direct_solar_radiation_f2_RW_24hrs\",\n",
    "    \"surface_solar_radiation_downwards_f2_delay_1hrs\", \"surface_solar_radiation_downwards_f2_RW_24hrs\",\n",
    "    \"snowfall_f1_delay_1hrs\", \"snowfall_f1_RW_24hrs\",\n",
    "    \"total_precipitation_f1_delay_1hrs\", \"total_precipitation_f1_RW_24hrs\",\n",
    "    \"snowfall_f2_delay_1hrs\", \"snowfall_f2_RW_24hrs\",\n",
    "    \"total_precipitation_f2_delay_1hrs\", \"total_precipitation_f2_RW_24hrs\",\n",
    "    \"10_metre_u_wind_component_f1_delay_1hrs\", \"10_metre_u_wind_component_f1_RW_24hrs\",\n",
    "    \"10_metre_v_wind_component_f1_delay_1hrs\", \"10_metre_v_wind_component_f1_RW_24hrs\",\n",
    "    \"10_metre_u_wind_component_f2_delay_1hrs\", \"10_metre_u_wind_component_f2_RW_24hrs\",\n",
    "    \"10_metre_v_wind_component_f2_delay_1hrs\", \"10_metre_v_wind_component_f2_RW_24hrs\",\n",
    "    \"cloudcover_low_f1_delay_1hrs\", \"cloudcover_low_f1_RW_24hrs\",\n",
    "    \"cloudcover_low_f2_delay_1hrs\", \"cloudcover_low_f2_RW_24hrs\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85a563da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T09:47:57.679591Z",
     "iopub.status.busy": "2024-07-21T09:47:57.679318Z",
     "iopub.status.idle": "2024-07-21T09:48:11.076917Z",
     "shell.execute_reply": "2024-07-21T09:48:11.076093Z"
    },
    "papermill": {
     "duration": 13.408841,
     "end_time": "2024-07-21T09:48:11.079281",
     "exception": false,
     "start_time": "2024-07-21T09:47:57.670440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "for (\n",
    "    df_new_test, \n",
    "    df_new_target, \n",
    "    df_new_client, \n",
    "    df_new_historical_weather,\n",
    "    df_new_forecast_weather, \n",
    "    df_new_electricity_prices, \n",
    "    df_new_gas_prices, \n",
    "    sample_prediction,\n",
    ") in iter_test:\n",
    "\n",
    "    for df in [df_new_test, \n",
    "           df_new_target, \n",
    "           df_new_client, \n",
    "           df_new_historical_weather,\n",
    "           df_new_forecast_weather, \n",
    "           df_new_electricity_prices, \n",
    "           df_new_gas_prices, \n",
    "           sample_prediction]:\n",
    "\n",
    "    \n",
    "        # Convert boolean columns to int64\n",
    "        bool_columns = df.select_dtypes(include='bool').columns\n",
    "        df[bool_columns] = df[bool_columns].astype('int64')\n",
    "    df_new_gas_prices['mean_price'] = df_new_gas_prices[['lowest_price_per_mwh', 'highest_price_per_mwh']].mean(axis=1)\n",
    "    df_new_gas_prices = df_new_gas_prices.drop(['lowest_price_per_mwh', 'highest_price_per_mwh'], axis=1) \n",
    "    df_new_test['date'] = df_new_test['prediction_datetime'].dt.date\n",
    "\n",
    "    client_df, gas_prices_df, electricity_prices_df, forecast_weather_df, historical_weather_df, revealed_targets_df, test_df = update_with_new_data(\n",
    "        client_df,\n",
    "        gas_prices_df,\n",
    "        electricity_prices_df,\n",
    "        forecast_weather_df,\n",
    "        historical_weather_df,\n",
    "        revealed_targets_df,\n",
    "        test_df,\n",
    "        df_new_client,\n",
    "        df_new_gas_prices,\n",
    "        df_new_electricity_prices,\n",
    "        df_new_forecast_weather,\n",
    "        df_new_historical_weather,\n",
    "        df_new_target,\n",
    "        df_new_test,\n",
    "    )\n",
    "    \n",
    "\n",
    "    dataframes = [client_df, gas_prices_df, electricity_prices_df, forecast_weather_df, historical_weather_df, revealed_targets_df, test_df]\n",
    "\n",
    "    for df in dataframes:\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "    gas_prices_df['origin_date'] = pd.to_datetime(gas_prices_df['origin_date'])\n",
    "    electricity_prices_df['origin_date'] = pd.to_datetime(electricity_prices_df['origin_date'])\n",
    "    forecast_weather_df['origin_datetime'] = pd.to_datetime(forecast_weather_df['origin_datetime'])\n",
    "    \n",
    "    if (df_new_test['currently_scored'] == 1).any():\n",
    "        processor = ForecastWeatherProcessor(forecast_weather_df)\n",
    "        processor.process_all_operations()\n",
    "        filtered_df = processor.filtered_forecast_df\n",
    "        non_filtered_df = processor.non_filtered_forecast_df\n",
    "        processor = HistorytWeatherProcessor(historical_weather_df)\n",
    "        processor.process_all_operations()\n",
    "        historical_weather_df1 = processor.historical_weather_df\n",
    "        revealed_targets_production, revealed_targets_consumption = split_revealed_targets(revealed_targets_df)\n",
    "        concatenator = ConcatenateDataFrames(test_df, electricity_prices_df, historical_weather_df)\n",
    "        result_df = concatenator.concatenate_dfs()        \n",
    "        result_df = encode_datetime_columns(result_df)\n",
    "        result_df = extract_columns(result_df, selected_columns)\n",
    "        result_df = normalize_columns(result_df, columns_to_normalize)\n",
    "        result_df = drop_first_n_days(result_df, 'prediction_datetime', n=3)\n",
    "        consumption_df, production_df = split_csv_by_consumption(result_df)\n",
    "\n",
    "        consumption_df = consumption_df.fillna(method='ffill')\n",
    "        production_df = production_df.fillna(method='ffill')\n",
    "        \n",
    "        unique_units = consumption_df['prediction_unit_id'].unique()\n",
    "        df_con = pd.concat([create_features_for_unit(consumption_df.copy(), unit_id) for unit_id in unique_units])\n",
    "        unique_units = production_df['prediction_unit_id'].unique()\n",
    "        df_pro = pd.concat([create_features_for_unit(production_df.copy(), unit_id) for unit_id in unique_units])\n",
    "\n",
    "\n",
    "        df_pro['l_espoir'] = df_pro['prediction_datetime'].dt.year.apply(calculate_l_espoir)        \n",
    "        df_con = df_con.drop('prediction_datetime', axis=1)  \n",
    "        df_pro = df_pro.drop('prediction_datetime', axis=1) \n",
    "        \n",
    "        row_ids_to_keep = sample_prediction['row_id'].tolist()\n",
    "        df_con_to_be_predicted = df_con[df_con['row_id'].isin(row_ids_to_keep)]\n",
    "        df_pro_to_be_predicted = df_pro[df_pro['row_id'].isin(row_ids_to_keep)]\n",
    "        \n",
    "\n",
    "        \n",
    "        sub_con = pd.DataFrame({'row_id': df_con_to_be_predicted['row_id'], 'target': None})\n",
    "        sub_pro = pd.DataFrame({'row_id': df_pro_to_be_predicted['row_id'], 'target': None})\n",
    "        \n",
    "        \n",
    "        #df_con_to_be_predicted.drop('prediction_unit_id', axis=1, inplace=True)\n",
    "        #df_pro_to_be_predicted.drop('prediction_unit_id', axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "        for feature in features:\n",
    "            if feature in df_pro_to_be_predicted.columns:\n",
    "                df_pro_to_be_predicted[feature] = 0\n",
    "            if feature in df_con_to_be_predicted.columns:\n",
    "                df_con_to_be_predicted[feature] = 0\n",
    "        \n",
    "        predictions_con = consumption_model.predict(df_con_to_be_predicted)\n",
    "        predictions_pro = production_model.predict(df_pro_to_be_predicted)\n",
    "        sub_con['target'] = predictions_con\n",
    "        sub_pro['target'] = predictions_pro\n",
    "        concatenated_df = pd.concat([sub_con, sub_pro], sort=False).sort_values(by='row_id', ascending=True).reset_index(drop=False)\n",
    "\n",
    "\n",
    "        sample_prediction['target'] = 0.0\n",
    "        merged_df = sample_prediction.merge(concatenated_df[['row_id', 'target']], on='row_id', how='left', suffixes=('_sample', '_concatenated'))\n",
    "        sample_prediction['target'] = np.where(merged_df['target_concatenated'].notna(), merged_df['target_concatenated'], sample_prediction['target'])  \n",
    "        sample_prediction['target'] = np.clip(sample_prediction['target'], 0, np.inf)\n",
    "\n",
    "        env.predict(sample_prediction)\n",
    "\n",
    "    else:\n",
    "        # Code to run when no element in df_new_test['currently_scored'] is equal to 1\n",
    "        sample_prediction['target'] = 0.0\n",
    "\n",
    "        env.predict(sample_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b4114",
   "metadata": {
    "papermill": {
     "duration": 0.008287,
     "end_time": "2024-07-21T09:48:11.096119",
     "exception": false,
     "start_time": "2024-07-21T09:48:11.087832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fee8d0",
   "metadata": {
    "papermill": {
     "duration": 0.008374,
     "end_time": "2024-07-21T09:48:11.112490",
     "exception": false,
     "start_time": "2024-07-21T09:48:11.104116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568294a",
   "metadata": {
    "papermill": {
     "duration": 0.007889,
     "end_time": "2024-07-21T09:48:11.128698",
     "exception": false,
     "start_time": "2024-07-21T09:48:11.120809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec59df80",
   "metadata": {
    "papermill": {
     "duration": 0.007761,
     "end_time": "2024-07-21T09:48:11.144485",
     "exception": false,
     "start_time": "2024-07-21T09:48:11.136724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c7ac9",
   "metadata": {
    "papermill": {
     "duration": 0.007964,
     "end_time": "2024-07-21T09:48:11.160372",
     "exception": false,
     "start_time": "2024-07-21T09:48:11.152408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4c13a",
   "metadata": {
    "papermill": {
     "duration": 0.007705,
     "end_time": "2024-07-21T09:48:11.176134",
     "exception": false,
     "start_time": "2024-07-21T09:48:11.168429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    },
    {
     "datasetId": 4012029,
     "sourceId": 6981281,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4297281,
     "sourceId": 7444466,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4301272,
     "sourceId": 7457586,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4373415,
     "sourceId": 7514798,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4337191,
     "sourceId": 7859345,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5419975,
     "sourceId": 8999379,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 50.107124,
   "end_time": "2024-07-21T09:48:12.305171",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-21T09:47:22.198047",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
